import json
import os
from collections import Counter

from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
from torchvision import transforms

from dataset.dataset import HazardDataset

# ĞŸÑƒÑ‚Ğ¸ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
images_folder = os.path.join(BASE_DIR, "../hazard_detection_data/drone/images/")
annotations_file = os.path.join(BASE_DIR, "../hazard_detection_data/drone/annotations/all_annotations.json")

# Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹
transforms_pipeline = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸
with open(annotations_file, "r") as f:
    data = json.load(f)

# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ `image_id -> category_id`
image_id_to_category = {ann["image_id"]: ann["category_id"] for ann in data["annotations"]}

# Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞµĞ¼ `image_id` Ğ¿Ğ¾ ĞºĞ»Ğ°ÑÑĞ°Ğ¼
hazardous_ids = [img["id"] for img in data["images"] if image_id_to_category.get(img["id"], 0) == 1]
non_hazardous_ids = [img["id"] for img in data["images"] if image_id_to_category.get(img["id"], 0) == 2]

# Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞµĞ¼ train/test ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ (10% Ğ² Ñ‚ĞµÑÑ‚)
hazardous_train, hazardous_test = train_test_split(hazardous_ids, test_size=0.1, random_state=42)
non_hazardous_train, non_hazardous_test = train_test_split(non_hazardous_ids, test_size=0.1, random_state=42)

# ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ train Ğ¸ test
train_ids = set(hazardous_train + non_hazardous_train)
test_ids = set(hazardous_test + non_hazardous_test)

# Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚
dataset = HazardDataset(images_folder, annotations_file, transform=transforms_pipeline)

train_dataset = [dataset[i] for i in range(len(dataset)) if dataset.image_ids[i] in train_ids]
test_dataset = [dataset[i] for i in range(len(dataset)) if dataset.image_ids[i] in test_ids]

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

train_labels = [label for _, label in train_dataset]
test_labels = [label for _, label in test_dataset]

class_counts_train = Counter(train_labels)
class_counts_test = Counter(test_labels)

print(f"âœ… Train size: {len(train_dataset)}, Test size: {len(test_dataset)}")
print(f"ğŸ“Š Class distribution in train dataset: {class_counts_train}")
print(f"ğŸ“Š Class distribution in test dataset: {class_counts_test}")
